{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09d6771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mstopwords\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('stopwords')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/mukundhjayapal/nltk_data'\n",
      "    - '/Users/mukundhjayapal/opt/anaconda3/nltk_data'\n",
      "    - '/Users/mukundhjayapal/opt/anaconda3/share/nltk_data'\n",
      "    - '/Users/mukundhjayapal/opt/anaconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/tkinter/__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/var/folders/0b/s0v82shn3550_2vgwnp6pvwh0000gn/T/ipykernel_6502/3156773970.py\", line 27, in evaluate\n",
      "    pipeline.fit(msg_train,label_train)\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 110, in _analyze\n",
      "    doc = analyzer(doc)\n",
      "  File \"/var/folders/0b/s0v82shn3550_2vgwnp6pvwh0000gn/T/ipykernel_6502/3156773970.py\", line 19, in process_text\n",
      "    mess = [word for word in mess.split() if word not in stopwords.words('english')]\n",
      "  File \"/var/folders/0b/s0v82shn3550_2vgwnp6pvwh0000gn/T/ipykernel_6502/3156773970.py\", line 19, in <listcomp>\n",
      "    mess = [word for word in mess.split() if word not in stopwords.words('english')]\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\", line 121, in __getattr__\n",
      "    self.__load()\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "  File \"/Users/mukundhjayapal/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mstopwords\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('stopwords')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/mukundhjayapal/nltk_data'\n",
      "    - '/Users/mukundhjayapal/opt/anaconda3/nltk_data'\n",
      "    - '/Users/mukundhjayapal/opt/anaconda3/share/nltk_data'\n",
      "    - '/Users/mukundhjayapal/opt/anaconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import messagebox as mbox\n",
    "from tkinter import simpledialog,ttk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "\n",
    "def process_text(mess):\n",
    "    mess = [word for word in mess if word not in punctuation]\n",
    "    mess = ''.join(mess)\n",
    "    mess = [word for word in mess.split() if word not in stopwords.words('english')]\n",
    "    return mess\n",
    "def evaluate():\n",
    "    global text\n",
    "    text=txt.get()\n",
    "    messages = pd.read_csv('SMSSpamCollection',sep='\\t',names=['label','message'])\n",
    "    msg_train,msg_test,label_train,label_test = train_test_split(messages['message'],messages['label'],test_size=0.3)\n",
    "    pipeline = Pipeline([('bow',CountVectorizer(analyzer=process_text)),('tfidf',TfidfTransformer()),('classifier',RandomForestClassifier())])\n",
    "    pipeline.fit(msg_train,label_train)\n",
    "    print(\"ok\")\n",
    "    print([f'{text}'])\n",
    "    p=pipeline.predict([f'{text}'])[0]\n",
    "    print(\"ok\")\n",
    "    print(p)\n",
    "          \n",
    "def main_window():\n",
    "     global root,txt\n",
    "     root=Tk()\n",
    "     root.title('Spam Detector')\n",
    "\n",
    "     root.geometry('1100x800')\n",
    "     bg=PhotoImage(file='enter.png')\n",
    "     Label(root,image=bg).place(relwidth=1,relheight=1)\n",
    "     txt=StringVar()\n",
    "     text=Entry(root,width=40,textvariable=txt).place(x=390,y=350)\n",
    "     btpic=PhotoImage(file='sub2.png')\n",
    "     b1=Button(root,text='Submit',padx=30,pady=15,bg='#000000',image=btpic,command=evaluate)\n",
    "     b1.place(x=500,y=390)\n",
    "     root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_window()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489cd567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
